<!DOCTYPE html>
<html>
    <head>
        <title>EBA - Puppet and Hiera</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            EBA - Puppet and Hiera
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    
                    <div id="main-content" class="wiki-content group">
                    <p><style type='text/css'>/*<![CDATA[*/
div.rbtoc1455124454862 {padding: 0px;}
div.rbtoc1455124454862 ul {list-style: circle;margin-left: 0px;}
div.rbtoc1455124454862 li {margin-left: 0px;padding-left: 0px;}

/*]]>*/</style><div class='toc-macro rbtoc1455124454862'>
<ul class='toc-indentation'>
<li><a href='#EBA-PuppetandHiera-Purpose'>Purpose</a></li>
<li><a href='#EBA-PuppetandHiera-PuppetEnterpriseComponents'>Puppet Enterprise Components</a></li>
<li><a href='#EBA-PuppetandHiera-PuppetEnterpriseOverview'>Puppet Enterprise Overview</a></li>
<li><a href='#EBA-PuppetandHiera-UsageinEBSAManagedEnvironments'>Usage in EBSA Managed Environments</a>
<ul class='toc-indentation'>
<li><a href='#EBA-PuppetandHiera-NodeClassification'>Node  Classification</a></li>
<li><a href='#EBA-PuppetandHiera-ModuleLayout'>Module Layout</a></li>
<li><a href='#EBA-PuppetandHiera-PuppetAgentdaemon'>Puppet Agent daemon</a></li>
<li><a href='#EBA-PuppetandHiera-CertificateSigning'>Certificate Signing</a></li>
</ul>
</li>
<li><a href='#EBA-PuppetandHiera-PuppetModuleDesign'>Puppet Module Design</a>
<ul class='toc-indentation'>
<li><a href='#EBA-PuppetandHiera-GuidingPrinciples'>Guiding Principles</a></li>
<li><a href='#EBA-PuppetandHiera-CustomFacts'>Custom Facts</a></li>
<li><a href='#EBA-PuppetandHiera-CoreOSConfiguration'>Core OS Configuration</a></li>
<li><a href='#EBA-PuppetandHiera-Monitoring'>Monitoring</a></li>
<li><a href='#EBA-PuppetandHiera-DNS'>DNS</a></li>
<li><a href='#EBA-PuppetandHiera-Firewall(IPTables)'>Firewall (IP Tables)</a></li>
</ul>
</li>
<li><a href='#EBA-PuppetandHiera-DynamicEnvironmentDataRepository(Hiera)'>Dynamic Environment Data Repository (Hiera)</a>
<ul class='toc-indentation'>
<li><a href='#EBA-PuppetandHiera-Manysources==&gt;oneoutcome'>Many sources ==&gt; one outcome</a></li>
<li><a href='#EBA-PuppetandHiera-Composition'>Composition</a></li>
<li><a href='#EBA-PuppetandHiera-Hieradatadelivery'>Hieradata delivery</a></li>
</ul>
</li>
<li><a href='#EBA-PuppetandHiera-DeploymentRepositories'>Deployment Repositories</a></li>
<li><a href='#EBA-PuppetandHiera-DeploymentProcess'>Deployment Process</a>
<ul class='toc-indentation'>
<li><a href='#EBA-PuppetandHiera-PuppetModules'>Puppet Modules</a></li>
<li><a href='#EBA-PuppetandHiera-PuppetHiera'>Puppet Hiera</a></li>
</ul>
</li>
<li><a href='#EBA-PuppetandHiera-BuildToolIntegration'>Build Tool Integration</a>
<ul class='toc-indentation'>
<li><a href='#EBA-PuppetandHiera-MCollective(Orchestration)'>MCollective (Orchestration)</a></li>
<li><a href='#EBA-PuppetandHiera-PuppetTags'>Puppet Tags</a></li>
<li><a href='#EBA-PuppetandHiera-Jenkinsplug-in'>Jenkins plug-in</a></li>
</ul>
</li>
</ul>
</div></p><h3 id="EBA-PuppetandHiera-Purpose">Purpose</h3><p>The purpose of this document is to explain how the Environment Configuration Controller (Puppet Enterprise) is used in EBA managed environments. This document starts off giving an overview of puppet and then how it has been implemented/integrated into the Environment Build and Management subsystem.</p><h3 id="EBA-PuppetandHiera-PuppetEnterpriseComponents">Puppet Enterprise Components</h3><p>Puppet Enterprise consists of following important components. A brief description given alongside the component.</p><p><u><strong>Puppet Server/Master:</strong></u> Puppet Master acts as a core component in a client/server distributed system.<br /><u><strong>Puppet Agent:</strong></u> Agent can be any server which is to be managed under puppet.<br /><strong><u>Puppet Enterprise Console:</u></strong> This is a dashboard/web gui provided puppet enterprise which lists agents, their state and other information about the agents.<br /><strong><u>PuppetDB:</u></strong> Puppet DB source stores facts &amp; catalogue information of nodes, Its improves the performance of catalogue compilation on puppet master.<br /><span style="color: rgb(0,0,0);"><u><strong>MCollective (MCo):</strong></u></span> The Marionette Collective, also known as MCollective, is a orchestration tool that executes administrative tasks on cluster of servers at a time.<br /><u><strong>Facter:</strong></u> This tool gathers facts about a node. ex: RAM, CPU cores &amp; IP address etc.</p><h3 id="EBA-PuppetandHiera-PuppetEnterpriseOverview">Puppet Enterprise Overview</h3><p>Puppet Enterprise will be deployed in a master-agent configuration and the agents will check in with the master every 30 minutes.</p><p>The diagram below shows the interactions between the agent and the master when the agent checks in.</p><p align="center"><span class="confluence-embedded-file-wrapper"><img class="confluence-embedded-image" src="attachments/65482098/65483423.png" data-image-src="attachments/65482098/65483423.png" data-unresolved-comment-count="0" data-linked-resource-id="65483423" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="puppet_master_agent_interaction_bau.png" data-base-url="https://<confluenceurl>" data-linked-resource-content-type="image/png" data-linked-resource-container-id="65482098" data-linked-resource-container-version="35"></span></p><p align="center"><strong>Figure 1 - Puppet Agent to Master Lifecycle</strong></p><ol><li>The agent requests the synchronisation of Facter plugins.  Facter, part of Puppet Enterprise, gathers facts about a host (e.g. hostname, domain name, IP address, number of CPUs, etc.).  The default set of facts that Facter provides can be expanded by writing Facter plugins.  Those plugins reside on the master and new and/or changed plugins are sent to the agent during this process at the start of every agent run.</li><li>The master sends any new and/or changed plugins back to the agent</li><li>With all of the Facter plugins now present on the agent machine, the agent gathers facts about the host and sends these in conjunction with its hostname to the master in a request for a list of all of the configuration items it needs to manage (a ‘catalogue’).</li><li>When the Puppet master receives a catalogue request, the first thing it needs to do is ‘classify’ the node; that is, using the facts submitted to it, it will determine what host is requesting the catalogue and what resources need to be managed on it.</li><li>Once the relevant classes have been identified, the master compiles a ‘catalogue’ and returns it back to the requesting agent.  The catalogue is a list of all of the resources on the agent’s machine that Puppet will manage.  The resources are defined in ‘classes’, and related classes are grouped in ‘modules’.</li><li>The agent then processes the catalogue, and queries the state of its host to determine if it needs to carry out any actions, i.e. whether the current state is different than that defined in the catalogue.  If any differences are determined, the agent performs corrective actions, which then bring the host in line with its defined state.</li><li>The agent then sends a report back to the master detailing what, if any, actions it performed, and whether those actions were successful or not.</li></ol><h3 id="EBA-PuppetandHiera-UsageinEBSAManagedEnvironments">Usage in EBSA Managed Environments</h3><h4 id="EBA-PuppetandHiera-NodeClassification">Node  Classification</h4><p>Node classification is the process by which a Puppet class (or set of classes) are assigned to a node.</p><p><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image" width="400" src="attachments/65482098/65487998.png" data-image-src="attachments/65482098/65487998.png" data-unresolved-comment-count="0" data-linked-resource-id="65487998" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="pandr.png" data-base-url="https://<confluenceurl>" data-linked-resource-content-type="image/png" data-linked-resource-container-id="65482098" data-linked-resource-container-version="35"></span></p><p>Nodes will be classified using a combination of Hiera <span style="color: rgb(255,0,0);"> </span> and the ‘Roles and Profiles’ module design pattern ((<a href="http://docs.puppetlabs.com/pe/latest/puppet_assign_configurations.html#assigning-configuration-data-with-role-and-profile-modules" class="external-link" rel="nofollow">http://docs.puppetlabs.com/pe/latest/puppet_assign_configurations.html#assigning-configuration-data-with-role-and-profile-m<span style="color: rgb(0,0,0);">odules</span></a><span style="color: rgb(0,0,0);">).  Using this pattern, Hiera is looked up, and returns the single ‘role’ module that needs to be assigned to the node.    That role class includes the various profile classes that make up that role.  Those profile classes, in turn, invoke whichever modules are necessary to deliver that functionality.  An example of a role is given below:</span></p><p><span style="color: rgb(0,0,0);">class role::web inherits role {<br />    include profile::apache<br />    include profile::memcached<br />}</span></p><p><span style="color: rgb(0,0,0);">As the example shows, role modules simply include the various profile classes that comprise the role.<br />Using class inheritance, all roles can include whatever resources are classed as ‘base’/’common’ (e.g. LDAP configuration, SSH access, and security hardening configuration):</span></p><p><span style="color: rgb(0,0,0);">class role {<br />    include profile::base<br />}</span></p><p><span style="color: rgb(0,0,0);">and define the node's definition in Hiera.<br /></span></p><p><span style="color: rgb(0,0,0);">classes:<br />  role::web</span></p><p><span style="color: rgb(0,0,0);">There are several reasons to adopt this method of node classification:</span></p><ol><li><span style="color: rgb(0,0,0);">Using Hiera alone prevents the use of modules that only declare ‘defined types’ rather than first-order classes.  Such ‘defined types’ need to be wrapped in a class in order to be instantiated and there is no mechanism in Hiera to support this.  The role and profile classes provide a natural location to instantiate suc</span>h defined types.</li><li>Using an External Node Classifier (ENC) such as the Puppet Enterprise Console is generally harder to automate as they are often implemented as Web Applications.  Using the GUI interfaces of an ENC, nodes and classes can be assigned to hostgroups.  A custom ENC can be written which supports a more dynamic classification system, however that would be another component to design, implement, integrate and test.</li></ol><h4 id="EBA-PuppetandHiera-ModuleLayout">Module Layout</h4><p>Module loca<span style="color: rgb(0,0,0);">tion is based on environments. The base location of the Puppet Master’s configuration directory is /etc/puppetlabs/puppet/, Puppet master config property, <em>modulepath = /etc/puppetlabs/puppet/environments/$environment/modules:/opt/puppet/share/puppet/modules</em></span></p><p>In order to support the deployment of different versions of modules and Hiera within different environments, the following directory structure will be maintained on each Puppet Master:</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><td class="confluenceTd"><div class="container" title="Hint: double-click to select code"><div class="line number1 index0 alt2"><code class="java plain">/etc/puppetlabs/code/environments</code></div><div class="line number2 index1 alt1"><code class="java spaces">                       </code><code class="java plain">/&lt;environment&gt;</code></div><div class="line number3 index2 alt2"><code class="java spaces">                          </code><code class="java plain">environment.conf</code></div><div class="line number4 index3 alt1"><code class="java spaces">                          </code><code class="java plain">/manifests</code></div><div class="line number5 index4 alt2"><code class="java spaces">                          </code><code class="java plain">/modules</code></div><div class="line number6 index5 alt1"><code class="java spaces">                       </code><code class="java plain">/&lt;environment&gt;</code></div><div class="line number7 index6 alt2"><code class="java spaces">                          </code><code class="java plain">environment.conf</code></div><div class="line number8 index7 alt1"><code class="java spaces">                          </code><code class="java plain">/manifests</code></div><div class="line number9 index8 alt2"><code class="java spaces">                          </code><code class="java plain">/modules</code></div><div class="line number10 index9 alt1"><code class="java spaces">                       </code><code class="java plain">...</code></div></div></td></tr><tr><td colspan="1" class="confluenceTd"><span style="color: rgb(0,0,0);">/&lt;environment&gt;</span></td><td colspan="1" class="confluenceTd"><span style="color: rgb(0,0,0);">Contains the modules and Hiera for a given environment. Where each &lt;environment&gt; directory is replaced by an environment name e.g. np-as-pjt3-dev1</span></td></tr><tr><td class="confluenceTd">environment.conf</td><td class="confluenceTd"><p><span style="color: rgb(0,0,0);">This file can override several settings whenever the Puppet master is serving nodes assigned to that environment.</span></p></td></tr><tr><td class="confluenceTd">/manifests</td><td class="confluenceTd"><p><span style="color: rgb(0,0,0);">Manifests are files containing Puppet code. They are standard text files saved with the <code>.pp</code> extension. Most manifests should be arranged into <a class="external-link" href="https://docs.puppetlabs.com/pe/latest/puppet_modules_manifests.html#puppet-modules" rel="nofollow"><span style="color: rgb(0,0,0);">modules</span></a>.</span></p></td></tr><tr><td class="confluenceTd">/modules</td><td class="confluenceTd"><p><span style="color: rgb(0,0,0);">Contains a collection of Puppet modules. Each module is stored within its own sub-directory which comprises of:</span></p><ul><li><p><span style="color: rgb(0,0,0);">manifests - contains all manifests (code organised into classes) in the module</span></p></li><li><p><span style="color: rgb(0,0,0);">files - contains static files, which managed nodes can download<br /></span></p></li><li><p><span style="color: rgb(0,0,0);">lib - contains Ruby plugins, like custom facts and custom resource types to Facter and Puppet</span></p></li><li><p><span style="color: rgb(0,0,0);">templates - contains templates, which the modules manifests can use</span></p></li><li><p><span style="color: rgb(0,0,0);">tests - contains examples shows how to declare the modules classes and defined types</span></p></li><li><p><span style="color: rgb(0,0,0);">spec - contains spec tests for any plugins in the lib directory</span></p></li></ul></td></tr></tbody></table></div><p>Each environment’s modules will be stored in a Git repository, under a branch specific to their environment.  This will enable new features and/or bug fixes to be easily merged between environments.</p><p>The puppet agent’s configuration will be changed by the guest OS customisation script such that when it checks in with the master, it correctly informs the master of which environment it resides in, so that it can use the correct version of the modules.</p><h4 id="EBA-PuppetandHiera-PuppetAgentdaemon">Puppet Agent daemon</h4><p class="Bodycopy">By default, the puppet agent will run in ‘apply’ mode (as opposed to ‘noop’ mode), which means that it will apply changes as and when it identifies them. This will result in any manual changes to a Puppet-managed resource (software package, configuration file, etc.) being automatically undone the next time the Puppet agent checks in with the Puppet master, reverting the node to its configuration-managed state.</p><p class="Bodycopy">However, running in that mode can cause issues when a new environment is being created; in a multi-tiered infrastructure, some resources will need to be updated across tiers in a well-defined order.  Puppet can only manage resource dependencies on an individual server basis (e.g. ensure the ‘ssh’ software is installed first, before trying to start the ‘ssh’ service).  Puppet cannot handle resource dependencies between servers (e.g. ensuring a database service is available on the database tier before trying to deploy an application on a server that requires database connectivity).</p><p class="Bodycopy">In order to handle that case, the orchestration software, MCo will request puppet agents to be run on specific servers, in a defined order, and only apply specific resources on each run. Once the environment has been created, the orchestration software will start all of the puppet agents in its ‘business as usual’ mode (i.e. check-in every 30 minutes and apply any changes it identifies).</p><h4 id="EBA-PuppetandHiera-CertificateSigning">Certificate Signing</h4><p class="Bodycopy">The Puppet Master and Puppet agent use SSL certificates to authorize connections.  When a node contacts the master for the first time, it sends a Certificate Signing Request (CSR) to the Puppet Master.  By default, the CSR will sit in a queue awaiting an administrator to log in, verify, and then sign the certificate manually.</p><p class="Bodycopy">Given the requirement to be able to automatically provision new environments, the certificate signing configuration will be changed to allow auto-signing of certificates for any node that identifies itself as being within the ipt.local domain.  Given the security around provisioning nodes within the cloud provider’s infrastructure, the level of trust placed on a node that is able to connect to the Puppet Master is very high, and therefore the threat to security posed by the auto-signing policy is considered low.</p><h3 id="EBA-PuppetandHiera-PuppetModuleDesign">Puppet Module Design</h3><h4 id="EBA-PuppetandHiera-GuidingPrinciples">Guiding Principles</h4><p class="Bodycopy">A lot of Puppet Modules for common functionality exist within the Puppet Forge (<a href="https://forge.puppetlabs.com/" class="external-link" rel="nofollow">https://forge.puppetlabs.com/</a>), which is a repository of modules written by the Puppet user community.  Much of the functionality that EBSA require can be fulfilled by using existing modules available on the Puppet Forge, so rather than expend effort writing such modules from scratch, modules from the Puppet Forge should be preferred.</p><p class="Bodycopy">As with any community repository, the quality of implementation can vary substantially.  As such, the following factors should be considered when selecting a module for any given function:</p><ol><li class="Bodycopy">Author: Puppetlabs maintain some modules on the Forge, and these should be preferred over other author’s implementations, all other factors being equal.</li><li class="Bodycopy">Flexibility: It shouldn’t be necessary to edit the code of the module in order to integrate it into the EBSA solution.  Instead, it should be possible to customise the module’s behaviour by passing parameters to it.</li><li class="Bodycopy">Documentation: It shouldn’t be necessary to read the module’s code in order to use it or customise its behaviour.  A well written module should include a README file containing both an overview of what functionality it provides, as well as clear usage examples.</li><li class="Bodycopy">Test Coverage: Automated unit test scripts can be written for Puppet modules, and these tests can be integrated into the wider code control &amp; continuous integration cycle.  A well written module should include comprehensive test scripts that exercise all of its code paths.</li></ol><h4 id="EBA-PuppetandHiera-CustomFacts">Custom Facts</h4><p>The custom facts that are required in order to meet the needs of the Hiera configuration, as well as being able to configure certain elements of the operating system, are provided by an in-house developed modules.  The examples of some of the  custom facts are given below:</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><td class="confluenceTd"><p><strong>Fact Name</strong></p></td><td class="confluenceTd"><p><strong>Description</strong></p></td></tr><tr><td class="confluenceTd"><p>default_gateway</p></td><td class="confluenceTd"><p>This fact calculates the default gateway address by inspecting the value of the server’s primary IP address (i.e. its management IP address). This is required in order to enable the ‘system’ module to be able to manage network interface configuration.</p><p>For most servers, it simply converts the last octet of the IP address to ‘1’, e.g. for a box whose management IP address is 10.9.4.2, its default gateway address will be 10.9.4.1</p><p>Management Zone Firewalls are unique, in that their default gateway address has the last 2 octets converted to ‘1’, e.g. for a firewall whose management IP address is 10.32.0.8, its default gateway address will be 10.32.1.1.</p></td></tr><tr><td class="confluenceTd"><p>ipaddress_ext</p></td><td class="confluenceTd"><p>In order for hosts to be reachable from outside of the vApp in which they reside, their external IP address needs to be advertised.  This is required for external DNS records, server monitoring.<br />For most servers, it simply converts the internal management IP address using the following calculation:</p><p>1<sup>st</sup> octet = unchanged<br />2<sup>nd</sup> octet = 2<sup>nd</sup> Octet + 32<br />3<sup>rd</sup> octet = vapp_id (see below)<br />4<sup>th</sup> octet = unchanged</p><p>Management Zone firewalls are unique, in that their management IP address is already an external IP.  As such, for these servers, the ipaddress_ext fact is simply set to the same as that provided by the ipaddress fact, which is provided by Facter.</p></td></tr><tr><td class="confluenceTd"><p>ipt_env</p></td><td class="confluenceTd"><p>This fact returns which environment a host resides in.  It is calculated by simply extracting the 2<sup>nd</sup> part of a hosts fully qualified domain name.  For example, given a host whose name is ‘apptzm01.st-sit1-ssb1.ipt.local’, its ipt_env fact will be ‘st-sit1-ssb1’.</p></td></tr><tr><td class="confluenceTd"><p>role</p></td><td class="confluenceTd"><p>This fact returns the role (i.e. function) that the server performs.  It is trivially derived from the first 3 characters of the server’s hostname.  For example, given a host whose name is apptzm01, its role fact will be ‘app’</p></td></tr><tr><td class="confluenceTd"><p>sec_comp</p></td><td class="confluenceTd"><p>This fact returns the security compartment that the host resides within, i.e. ‘st’, ‘pt’ or ‘pp’.  It is trivially derived from the first 2 characters of a server’s domain name.  For example, given a host whose domain is ‘st-sit1-ssb1.ipt.local’, its sec_comp fact will be ‘st’.</p><p>This fact is useful for configuring services that reside within a security compartment’s tooling environment (e.g. NTP or Puppet).</p></td></tr><tr><td class="confluenceTd"><p>vapp_id</p></td><td class="confluenceTd"><p>The vapp_id is used by the ipaddress_ext fact (see above).</p><p>On management zone firewalls, the vapp_id fact is trivially derived from the 3<sup>rd</sup> octet of its management network interface’s IP address.</p><p>On all other servers, the vapp_id fact is calculated by performing a DNS lookup against the management zone firewall’s hostname, i.e. ‘mzfmzm01’ and then extracting the 3<sup>rd</sup> octet of that IP address.</p></td></tr></tbody></table></div><h4 id="EBA-PuppetandHiera-CoreOSConfiguration">Core OS Configuration</h4><p>Core OS configuration involves package installation, configuration file manipulation, system service management and local user &amp; group management.  Whilst Puppet has native resources that manage all of those configuration items, using those would require writing a class particular to the item needing to be managed, and therefore also another module to contain that class.  Instead, the ‘erwbgy/system’ (<a href="https://forge.puppetlabs.com/erwbgy/system" class="external-link" rel="nofollow">https://forge.puppetlabs.com/erwbgy/system</a>) module provides a wrapper around those resources, which allows such configuration items to be defined in Hiera data.  It provides the following features:</p><ul><li>augeas: apply file changes using the augeas tool</li><li>crontabs: set user crontab entries</li><li>execs: run idempotent external commands</li><li>facts: set custom facts</li><li>files: create/update files or directories</li><li>groups: manage entries in /etc/group</li><li>hosts: manage entries in /etc/hosts</li><li>limits: manage entries in /etc/security/limits.conf</li><li>mail: manage entries in /etc/aliases or set a relay host</li><li>mounts: manage entries in /etc/fstab</li><li>network: configure basic networking and dns</li><li>ntp: configure NTP servers in /etc/ntp.conf</li><li>packages: manage system packages</li><li>schedules: determine when resource config should not be applied and how often</li><li>services: manage system services</li><li>sshd: manage configuration in /etc/ssh/sshd_config including subsystems like sftp</li><li>sysconfig: manage files under /etc/sysconfig: clock, i18n, keyboard, puppet-dashboard, puppet, puppetmaster, selinux</li><li>sysctl: manage entries in /etc/sysctl.conf</li><li>templates: create files from ERB templates</li><li>users: manage users in /etc/passwd and /etc/shadow</li><li>yumgroups: manage system package groups</li><li>yumrepos: manage yum repository files under /etc/yum.repos.d</li></ul><h4 id="EBA-PuppetandHiera-Monitoring">Monitoring</h4><p>Zabbix is being used to monitor server availability, health and performance.  The ‘fiddyspence/zabbix’ (<a href="https://forge.puppetlabs.com/fiddyspence/zabbix/license" class="external-link" rel="nofollow">https://forge.puppetlabs.com/fiddyspence/zabbix/license</a>) module will be used to configure the Zabbix server and the agents that reside on each of the hosts in the EBSA-managed environments.  The zabbix module uses the Zabbix API to automatically add hosts and hostgroups exported from other servers to the Zabbix server.  The base profile class will therefore include an exported zabbix_host resource so that every server exports its host details, which can then be collected by the Zabbix server.  As duplicate resources cannot be declared (doing so results in a catalogue compilation error), the zabbix_hostgroup for each environment can only be declared once.  As such, the management firewall profile will be the only one that exports the environment’s zabbix_hostgroup resource for collection by the Zabbix server.</p><h4 id="EBA-PuppetandHiera-DNS">DNS</h4><p><span style="color: rgb(0,0,0);">dnsmasq is a lightweight and easy to administer DNS server, which will be used on the management firewall servers to provide vApp-internal DNS resolution, as well as on the tooling vApp’s management server to provide external/inter-vApp DNS resolution.  The ‘saz/dnsmasq’ (<a href="https://forge.puppetlabs.com/saz/dnsmasq" class="external-link" rel="nofollow">https://forge.puppetlabs.com/saz/dnsmasq</a>) module from the Puppet Forge enables the installation and configuration of the dnsmasq package.  dnsmasq uses the /etc/hosts file on the server to provide hostname and IP resolution, so that file also needs to be managed by Puppet.</span></p><h4 id="EBA-PuppetandHiera-Firewall(IPTables)">Firewall (IP Tables)</h4><p>Host based firewalls on each of the non-MZF/non-DZF servers will use the ‘erwbgy/iptables’ (<a href="https://forge.puppetlabs.com/erwbgy/iptables" class="external-link" rel="nofollow">https://forge.puppetlabs.com/erwbgy/iptables</a>) module.  This provides the ability to define ports to allow access in to Hiera, and uses that Hiera data to generate the relevant rules in IPTables configuration file, then restarts the firewall process.</p><p>The requirements for the MZF and DZF boxes are more complex as they need to contain NATing and forwarding rules to allow traffic to pass through them.  None of the modules available on the Puppet Forge supported those more complex rulesets at the time EBSA started.  As such, a template iptables config file was written which provides all of the necessary rules, using some custom facts in order to determine the correct network address to translate.</p><h3 id="EBA-PuppetandHiera-DynamicEnvironmentDataRepository(Hiera)">Dynamic Environment Data Repository (Hiera)</h3><p>Data that defines the configuration of an environment, in terms of which middleware components are deployed, which version(s) of code are deployed, etc., will be held in a hierarchical database, Hiera.  Hiera is a component of the Puppet Enterprise suite of tools and is available on any host on which the Puppet agent has been installed.  Whilst Hiera supports a number of datastore backends (database, LDAP, etc.) in order to control changes accurately, its default YAML file based backend will be used.  The YAML files that compose the Hiera database will be stored in a Git repository so that changes to environments over time can be tracked.</p><h4 id="EBA-PuppetandHiera-Manysources==&gt;oneoutcome">Many sources ==&gt; one outcome</h4><p>Whichever way we organise our environment configuration we will never get past the fact that on the development side there are multiple contributors each concerned with their own area and on the execution side there is a single entity which puppet and mcollective read and use.  Whatever process we employ to collect changes to the hieradata needs to recognize and deal with these two fundamental facts (multiple contributors, single runtime entity).</p><p>The following diagram shows a progression from the contributors to a running set of hieradata<span class="gliffy-container  lockpointConfigured"> </span><span class="gliffy-container  lockpointConfigured"> </span></p><p>

<map id="gliffy-map-65491389-6447" name="gliffy-map-65491389-6447"></map>
<table width="100%" class="gliffy-macro-table">
    <tr>
        <td >
            <table class="gliffy-macro-inner-table">
                <caption align="bottom">
 </caption>

                <tr>
                    <td>
                        <img style="border: none; width: 1107px;" usemap="#gliffy-map-65491389-6447" src="attachments/65488099/65491390.png" alt="" class="gliffy-macro-image"/>
                    </td>
                </tr>
            </table>
 
        </td>
    </tr>
</table>


</p><h4 id="EBA-PuppetandHiera-Composition">Composition</h4><p>In this strategy we can compose the final structure of the hieradata by combining files which come from different sources into a unified directory structure (like an SQL union). Each set of files brought in from different source needs its own level in the hieradata hierarchy. This is achieved with either an agreed suffix or prefix on the filename or and agreed folder.  The diagram below shows this being achieved with folders.  The variables being overridden are the fqdn and role names. If the same path appears in more than one source then the file from one source (represented by the duplicated path) will overwrite the file from the other source.</p><p>

<map id="gliffy-map-65491397-266" name="gliffy-map-65491397-266"></map>
<table width="100%" class="gliffy-macro-table">
    <tr>
        <td >
            <table class="gliffy-macro-inner-table">
                <caption align="bottom">
 </caption>

                <tr>
                    <td>
                        <img style="border: none; width: 1140px;" usemap="#gliffy-map-65491397-266" src="attachments/65488099/65491398.png" alt="" class="gliffy-macro-image"/>
                    </td>
                </tr>
            </table>
 
        </td>
    </tr>
</table>


</p><h4 id="EBA-PuppetandHiera-Hieradatadelivery">Hieradata delivery</h4><p>At the moment hieradata is delivered through a variety of mechanisms and it is important to reduce these into a smaller set which is consistent with strategic patterns. The diagram below shows our as-is picture mixed in with a view of the future.</p><p>

<map id="gliffy-map-65491524-5204" name="gliffy-map-65491524-5204"></map>
<table width="100%" class="gliffy-macro-table">
    <tr>
        <td >
            <table class="gliffy-macro-inner-table">
                <caption align="bottom">
 </caption>

                <tr>
                    <td>
                        <img style="border: none; width: 1341px;" usemap="#gliffy-map-65491524-5204" src="attachments/65488099/65491525.png" alt="" class="gliffy-macro-image"/>
                    </td>
                </tr>
            </table>
 
        </td>
    </tr>
</table>


</p><p>There are predefined hieradata templates, and with the use of Apache Velocity (<a href="https://velocity.apache.org/engine/releases/velocity-1.7/user-guide.html" class="external-link" rel="nofollow">https://velocity.apache.org/engine/releases/velocity-1.7/user-guide.html</a>), the templates are manifested into actual yaml files during the build process.</p><h3 id="EBA-PuppetandHiera-DeploymentRepositories">Deployment Repositories</h3><p>In order to provide a consistent method of deploying software artefacts to target virtual machines, the Red Hat Package Manager (RPM) format will be used.  RPMs are the standard packaging format for installing packages on Red Hat Enterprise Linux (RHEL) and CentOS operating systems.  As these are the only 2 operating systems currently used throughout the EBSA managed environments, all package installations will be provided in RPM format.  This includes base OS software, COTS software, operating system and COTS patches, and application code deployments.</p><p>YUM (Yellowdog Update Manager) is the standard RPM package manager for RHEL and CentOS operating systems, and as such will be used to stage RPMs ready for deployment to target VMs.  RPMs will be mastered within a single Nexus repository, and when they need to be staged ready for deployment, will be copied into the relevant YUM repository.  At deployment time, target VMs will pull the RPMs from the relevant YUM repository, then install the package.</p><h3 id="EBA-PuppetandHiera-DeploymentProcess">Deployment Process</h3><h4 id="EBA-PuppetandHiera-PuppetModules">Puppet Modules</h4><p>The following diagram shows the puppet module promotion from it's development phase to Live. It goes through the initial commit to DEV branch then testing, then into a Workflow based CI.</p><p>

<map id="gliffy-map-65491436-5633" name="gliffy-map-65491436-5633"></map>
<table width="100%" class="gliffy-macro-table">
    <tr>
        <td >
            <table class="gliffy-macro-inner-table">
                <caption align="bottom">
 </caption>

                <tr>
                    <td>
                        <img style="border: none; width: 616px;" usemap="#gliffy-map-65491436-5633" src="attachments/65488099/65491437.png" alt="" class="gliffy-macro-image"/>
                    </td>
                </tr>
            </table>
 
        </td>
    </tr>
</table>


</p><h4 id="EBA-PuppetandHiera-PuppetHiera">Puppet Hiera</h4><p>Hieradata should not be promoted between environments.  It should only contain a small number of well understood environment specific configuration values e.g. IP addresses which should be directly deployed to target environments, and not promoted between them (as they will always be different).</p><p>IPT has not followed these best practices and as such a lot of environment agnostic information has been put into the hiera files.  This has been for a number of reasons:</p><ul><li>It is easier to code environment agnostic information into Puppet Hiera (text in yaml) than it is to code it into a Puppet Module (structured Ruby code)</li><li>Nothing technically prevents DevOps from adding environment agnostic information into Puppet Hiera, can only be governed through reviews and strong coding standards for Puppet Hiera</li></ul><p>This has resulted in the following types of data existing within each Hiera file:</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><td class="confluenceTd">EBSA Environment Specific</td><td class="confluenceTd">Managing the configuration of the base host services and structure e.g. managing firewall rules, network routing and disk layout</td></tr><tr><td class="confluenceTd">Project Environment Specific</td><td class="confluenceTd">Managing the configuration of the project specific middleware and application e.g. URLs, hostnames</td></tr><tr><td class="confluenceTd">Environment Agnostic</td><td class="confluenceTd">Configuration that does not change between environments e.g. creating directories, giving directories permissions etc.</td></tr></tbody></table></div><p>Having environment agnostic information within the Puppet Hiera files forces those files to be promoted between the environments and non-environment specific differences to emerge between environments.</p><p>The following diagram shows the current process of promoting Puppet Hiera changes through the IPT environments.</p><p>

<map id="gliffy-map-65491443-3705" name="gliffy-map-65491443-3705"></map>
<table width="100%" class="gliffy-macro-table">
    <tr>
        <td >
            <table class="gliffy-macro-inner-table">
                <caption align="bottom">
 </caption>

                <tr>
                    <td>
                        <img style="border: none; width: 784px;" usemap="#gliffy-map-65491443-3705" src="attachments/65488099/65491444.png" alt="" class="gliffy-macro-image"/>
                    </td>
                </tr>
            </table>
 
        </td>
    </tr>
</table>


</p><h3 id="EBA-PuppetandHiera-BuildToolIntegration">Build Tool Integration</h3><p>When a new environment is provisioned, configuration is rolled out in an order, that is making the Management Firewalls ready before the application firewalls and the nonMZF vms etc. This is orchestrated by MCollective using Puppet tags. Each MCo command again issued by jenkins as part of Build tool.</p><h4 id="EBA-PuppetandHiera-MCollective(Orchestration)">MCollective (Orchestration)</h4><p>mco command line tool is used from the MCo workstation to execute puppet runs. An example of command is given below.</p><p>/opt/puppet/bin/mco rpc -t 3600 gonzo run -F domain=ab-zyz-dazo.ipt.ho.local -S 'fqdn=pqr.ab-xyz-dazo.ipt.ho.local' <strong>tags=mwdeploy</strong> --verbose</p><p>In the example above, we are selecting the hosts that match domain, fqdn fact variables, and then executing/setting up the resources that match &quot;mwdeploy&quot; tag.</p><h4 id="EBA-PuppetandHiera-PuppetTags">Puppet Tags</h4><p>Every Puppet resource automatically gets some tags by default. For example for a file resource, the tags &quot;file&quot; &amp; class name are automatically assigned. If you explicitly tag a resource, it will be available to puppet, and it can be used to classify.</p><p>Tags can be used to associate a class or within a resource etc. Following is a resource definitions in the form hiera yaml.</p><p>ebsa::firewall::input:<br />    monfm:<br />        port:<br />        - 10050<br />        - 10051<br />        - 10052<br />        - 44446<br />        - 44445<br />        - 80<br />        <strong>tag: osconfig</strong></p><p>system::packages:<br />    jdk1.8.0_31:<br />        ensure: latest<br />        <strong>tag: mwdeploy</strong><br />    gen-bin-tomcat8:<br />        ensure: latest<br />        require: Package[jdk1.8.0_31]<br />       <strong> tag: mwdeploy</strong></p><p>There are following tags defined in the EBSA platform,</p><ul><li>osconfig</li><li>ospatch</li><li>mwdeploy</li><li>mwpatch</li><li>mwconfig</li><li>appdeploy</li><li>apppatch</li><li>appconfig</li></ul><h4 id="EBA-PuppetandHiera-Jenkinsplug-in">Jenkins plug-in</h4><p>In order to facilitate MCo commands at relevant stage of the build process, we have a jenkins job template. It takes some mandatory parameters (like facts, tags etc) and uses ssh to run the mco command.</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><td class="confluenceTd"><p><strong>Action Name<br /></strong></p></td><td class="confluenceTd"><p><strong>Description</strong></p></td><td class="confluenceTd"><p><strong>Parameters</strong></p></td></tr><tr><td class="confluenceTd"><p>run_puppet</p></td><td class="confluenceTd"><p>Sends a request to MCollective which in turn requests a puppet agent run on a server or a set of servers within an environment.</p></td><td class="confluenceTd"><ul><li>puppet_master</li><li>jumphosts</li><li>sshopts (ssh options)</li><li>domain</li><li>params (list of other mco parameters)</li></ul></td></tr></tbody></table></div>
                    </div>

                                        
                    
                 
                </div>             </div> 
            
        </div>     </body>
</html>
